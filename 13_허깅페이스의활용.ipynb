{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fce21c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "546422bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"HUGGING_FACE_TOKEN\" 이라는 이름으로 저장한 토큰 값을 가져옵니다.\n",
    "# HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af1c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f40672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\torch_cuda_transformer_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Admin\\miniconda3\\envs\\torch_cuda_transformer_env\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a good Friday.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-ko-en\")\n",
    "\n",
    "korean_text = \"오늘은 즐거운 금요일이에요\"\n",
    "\n",
    "result = translator(korean_text)\n",
    "\n",
    "print(result[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4b8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"hf-inference\",\n",
    "    api_key=os.environ[\"hf_token\"],\n",
    ")\n",
    "\n",
    "result = client.translation(\n",
    "    \"오늘은 즐거운 금요일이에요\",\n",
    "    model=\"Helsinki-NLP/opus-mt-ko-en\",\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441a6fe",
   "metadata": {},
   "source": [
    "# pipline()\n",
    "task: 텍스트 기반한 감성분석, 분류(제로샷), 텍스트생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\torch_cuda_transformer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--tabularisai--multilingual-sentiment-analysis. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Neutral', 'score': 0.6137834787368774}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\",\"tabularisai/multilingual-sentiment-analysis\") # task 지정\n",
    "english_text = \"hello\"\n",
    "result = classifier(english_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e107a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\torch_cuda_transformer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"fill-mask\",\"google-bert/bert-base-multilingual-cased\") # task 지정\n",
    "english_text = \"안녕하세요 나는 [MASK] 모델입니다.\"\n",
    "result = classifier(english_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d0760",
   "metadata": {},
   "source": [
    "# \"question-answering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"ner\" 개체명 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7267fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TV가 없는 집도 많아지고 미디어의 혜 택을 누릴 수 있는 방법은 늘어났다.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"summarization\" 문장요약\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n",
    "\n",
    "text = \"과거를 떠올려보자. 방송을 보던 우리의 모습을. 독보적인 매체는 TV였다. 온 가족이 둘러앉아 TV를 봤다. 간혹 가족들끼리 뉴스와 드라마, 예능 프로그램을 둘러싸고 리모컨 쟁탈전이 벌어지기도  했다. 각자 선호하는 프로그램을 ‘본방’으로 보기 위한 싸움이었다. TV가 한 대인지 두 대인지 여부도 그래서 중요했다. 지금은 어떤가. ‘안방극장’이라는 말은 옛말이 됐다. TV가 없는 집도 많다. 미디어의 혜 택을 누릴 수 있는 방법은 늘어났다. 각자의 방에서 각자의 휴대폰으로, 노트북으로, 태블릿으로 콘텐츠 를 즐긴다.\"\n",
    "\n",
    "raw_input_ids = tokenizer.encode(text)\n",
    "input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n",
    "\n",
    "summary_ids = model.generate(torch.tensor([input_ids]))\n",
    "tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"translation\" 한글->영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c16acc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "several hands are shown with fingers pointing at each other\n"
     ]
    }
   ],
   "source": [
    "# \"image-to-text\"\n",
    "# 모델명: Salesforce/blip-image-captioning-large\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# 이미지 캡셔닝 파이프라인 생성\n",
    "# device=0을 추가하면 GPU를 사용해 더 빨라집니다 (GPU가 있는 경우).\n",
    "captioner = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\", device=0)\n",
    "\n",
    "# 웹에서 이미지 불러오기\n",
    "local_image_path = \"sample.jpg\"\n",
    "image = Image.open(local_image_path)\n",
    "\n",
    "# 캡션 생성 실행\n",
    "result = captioner(image)\n",
    "\n",
    "# 결과 출력\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning을 위한 AutoCalss 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a035a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\torch_cuda_transformer_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('imdb')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bebd096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저 -모델\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "def preprocess_f(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "tokenized_datasets = dataset.map(preprocess_f, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc984e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a372d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정\n",
    "import os\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "tr_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    # --- 속도 향상을 위한 핵심 옵션들 ---\n",
    "    fp16=True,  # 최신 GPU 성능 활용 (혼합 정밀도)\n",
    "    dataloader_num_workers=4\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=tr_args,\n",
    "    train_dataset=tokenized_datasets['train'], \n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df257e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1411' max='7815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1411/7815 05:23 < 24:31, 4.35 it/s, Epoch 0.90/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.321500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.253800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5624391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가\n",
    "trainer.evaluate()\n",
    "# 저장\n",
    "model_path = './fine_tuned_distilbert_imdb'\n",
    "model.save_pretrained(model_path)\n",
    "tokenized_datasets.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e94cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장모델 로드\n",
    "# 토크나이저로드 \n",
    "# 모델 로드\n",
    "# 모델.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장모델로 추론하기\n",
    "input_text = 'This movie was absolutely fantastic!'\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors='pt') # python형식으로 텐서를 리턴 \n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():# 역전파x, 추론 oonly\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "predict_clas_id = torch.argmax(outputs.logits, dim =-1).item()\n",
    "print(f'예측 결과: {predict_clas_id}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8fced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 4060 Laptop GPU', major=8, minor=9, total_memory=8187MB, multi_processor_count=24, uuid=6a8f1015-3397-dda8-60dc-f77620240ba6, L2_cache_size=32MB)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_per_process_memory_fraction(0.5, device=device) \n",
    "print(torch.cuda.get_device_properties(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
